{"channeldata":{"activate.d":false,"binary_prefix":true,"deactivate.d":false,"description":null,"dev_url":null,"doc_source_url":null,"doc_url":null,"home":"https://lincolnmullen.com/software/tokenizers/","icon_hash":null,"icon_url":null,"identifiers":null,"keywords":null,"license":"MIT","post_link":false,"pre_link":false,"pre_unlink":false,"recipe_origin":null,"run_exports":{},"source_git_url":null,"source_url":["https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.2.1.tar.gz","https://cran.r-project.org/src/contrib/tokenizers_0.2.1.tar.gz"],"subdirs":["osx-64"],"summary":"Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'.","tags":null,"text_prefix":false,"timestamp":1594728825,"version":"0.1.4"},"channeldata_version":1,"feedstock":null,"labels":["main"],"package":"r-tokenizers-0.1.4-r3.4.1_0.tar.bz2","repodata":{"arch":"x86_64","build":"r3.4.1_0","build_number":0,"depends":["r-base 3.4.1*","r-rcpp >=0.12.3","r-snowballc >=0.5.1","r-stringi >=1.0.1"],"license":"MIT","license_family":"MIT","md5":"5d0042785fa050e7b520e30c4d80b3de","name":"r-tokenizers","platform":"osx","sha256":"6913901f7c5b35926b397e23be0e8a4a37738a42f626e6a187b614dbce6ed490","size":113391,"subdir":"osx-64","version":"0.1.4"},"repodata_version":1,"subdir":"osx-64","url":"https://conda.anaconda.org/conda-forge/osx-64/r-tokenizers-0.1.4-r3.4.1_0.tar.bz2"}