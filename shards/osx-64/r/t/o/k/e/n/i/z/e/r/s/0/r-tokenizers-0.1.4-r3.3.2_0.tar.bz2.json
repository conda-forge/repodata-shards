{"channeldata":{"activate.d":false,"binary_prefix":true,"deactivate.d":false,"description":null,"dev_url":null,"doc_source_url":null,"doc_url":null,"home":"https://lincolnmullen.com/software/tokenizers/","icon_hash":null,"icon_url":null,"identifiers":null,"keywords":null,"license":"MIT","post_link":false,"pre_link":false,"pre_unlink":false,"recipe_origin":null,"run_exports":{},"source_git_url":null,"source_url":["https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.2.1.tar.gz","https://cran.r-project.org/src/contrib/tokenizers_0.2.1.tar.gz"],"subdirs":["osx-64"],"summary":"Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'.","tags":null,"text_prefix":false,"timestamp":1594728825,"version":"0.1.4"},"channeldata_version":1,"feedstock":null,"labels":["main"],"package":"r-tokenizers-0.1.4-r3.3.2_0.tar.bz2","repodata":{"arch":"x86_64","build":"r3.3.2_0","build_number":0,"depends":["r-base 3.3.2*","r-rcpp >=0.12.3","r-snowballc >=0.5.1","r-stringi >=1.0.1"],"license":"MIT","license_family":"MIT","md5":"ae5163c751b450142ac76ec41910ec9f","name":"r-tokenizers","platform":"osx","sha256":"56cd023c8201d01275c6e74804266765476f6b34384b9059d3c14a8a8b68a33b","size":95532,"subdir":"osx-64","version":"0.1.4"},"repodata_version":1,"subdir":"osx-64","url":"https://conda.anaconda.org/conda-forge/osx-64/r-tokenizers-0.1.4-r3.3.2_0.tar.bz2"}