{
  "channeldata": {
    "activate.d": false,
    "binary_prefix": false,
    "deactivate.d": false,
    "description": "Einsum is a very powerful function for contracting tensors of arbitrary dimension and index. However, it is only optimized to contract two terms at a time resulting in non-optimal scaling. For example, let us examine the following index transformation: `M_{pqrs} = C_{pi} C_{qj} I_{ijkl} C_{rk} C_{sl}` We can then develop two seperate implementations that produce the same result: ```python N = 10 C = np.random.rand(N, N) I = np.random.rand(N, N, N, N) def naive(I, C):     return np.einsum('pi,qj,ijkl,rk,sl->pqrs', C, C, I, C, C)  def optimized(I, C):     K = np.einsum('pi,ijkl->pjkl', C, I)     K = np.einsum('qj,pjkl->pqkl', C, K)     K = np.einsum('rk,pqkl->pqrl', C, K)     K = np.einsum('sl,pqrl->pqrs', C, K)     return K ``` The einsum function does not consider building intermediate arrays; therefore, helping einsum out by building these intermediate arrays can result in a considerable cost savings even for small N (N=10): ```python np.allclose(naive(I, C), optimized(I, C)) True %timeit naive(I, C) 1 loops, best of 3: 934 ms per loop %timeit optimized(I, C) 1000 loops, best of 3: 527 \u00B5s per loop ``` A 2000 fold speed up for 4 extra lines of code! This contraction can be further complicated by considering that the shape of the C matrices need not be the same, in this case the ordering in which the indices are transformed matters greatly. Logic can be built that optimizes the ordering; however, this is a lot of time and effort for a single expression. The opt_einsum package is a drop in replacement for the np.einsum function and can handle all of this logic for you: ```python from opt_einsum import contract %timeit contract('pi,qj,ijkl,rk,sl->pqrs', C, C, I, C, C) 1000 loops, best of 3: 324 \u00B5s per loop ``` The above will automatically find the optimal contraction order, in this case identical to that of the optimized function above, and compute the products for you. In this case, it even uses `np.dot` under the hood to exploit any vendor BLAS functionality that your NumPy build has!",
    "dev_url": "https://github.com/dgasmith/opt_einsum",
    "doc_source_url": null,
    "doc_url": null,
    "home": "http://github.com/dgasmith/opt_einsum",
    "icon_hash": null,
    "icon_url": null,
    "identifiers": null,
    "keywords": null,
    "license": "MIT",
    "post_link": false,
    "pre_link": false,
    "pre_unlink": false,
    "recipe_origin": null,
    "run_exports": {},
    "source_git_url": null,
    "source_url": "https://github.com/dgasmith/opt_einsum/archive/v1.0.1.tar.gz",
    "subdirs": [
      "osx-64"
    ],
    "summary": "A contraction optimizer for the NumPy Einsum function.",
    "tags": null,
    "text_prefix": false,
    "timestamp": 0,
    "version": "1.0.1"
  },
  "channeldata_version": 1,
  "feedstock": null,
  "labels": [
    "main",
    "cf202003",
    "cf201901"
  ],
  "package": "opt_einsum-1.0.1-py27_0.tar.bz2",
  "repodata": {
    "build": "py27_0",
    "build_number": 0,
    "depends": [
      "numpy",
      "python 2.7*"
    ],
    "license": "MIT",
    "license_family": "MIT",
    "md5": "7ebb4406232c7817631ae36f7526a02a",
    "name": "opt_einsum",
    "sha256": "e8905b48312e3654582e8af7bb0fa35a7b84936e6692871c8383f5234b68f37f",
    "size": 22249,
    "subdir": "osx-64",
    "version": "1.0.1"
  },
  "repodata_version": 1,
  "subdir": "osx-64",
  "url": "https://conda.anaconda.org/conda-forge/osx-64/opt_einsum-1.0.1-py27_0.tar.bz2"
}