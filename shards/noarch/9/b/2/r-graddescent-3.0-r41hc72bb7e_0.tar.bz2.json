{
  "channeldata": {
    "activate.d": false,
    "binary_prefix": false,
    "deactivate.d": false,
    "description": null,
    "dev_url": null,
    "doc_source_url": null,
    "doc_url": null,
    "home": "https://github.com/drizzersilverberg/gradDescentR",
    "icon_hash": null,
    "icon_url": null,
    "identifiers": null,
    "keywords": null,
    "license": "GPL-2.0-or-later",
    "post_link": false,
    "pre_link": false,
    "pre_unlink": false,
    "recipe_origin": null,
    "run_exports": {},
    "source_git_url": null,
    "source_url": [
      "https://cran.r-project.org/src/contrib/Archive/gradDescent/gradDescent_3.0.tar.gz",
      "https://cran.r-project.org/src/contrib/gradDescent_3.0.tar.gz"
    ],
    "subdirs": [
      "noarch"
    ],
    "summary": "An implementation of various learning algorithms based on Gradient Descent for dealing with regression tasks. The variants of gradient descent algorithm are : Mini-Batch Gradient Descent (MBGD), which is an optimization to use training data partially to reduce the computation load. Stochastic Gradient Descent (SGD), which is an optimization to use a random data in learning to reduce the computation load drastically. Stochastic Average Gradient (SAG), which is a SGD-based algorithm to minimize stochastic step to average. Momentum Gradient Descent (MGD), which is an optimization to speed-up gradient descent learning. Accelerated Gradient Descent (AGD), which is an optimization to accelerate gradient descent learning. Adagrad, which is a gradient-descent-based algorithm that accumulate previous cost to do adaptive learning. Adadelta, which is a gradient-descent-based algorithm that use hessian approximation to do adaptive learning. RMSprop, which is a gradient-descent-based algorithm that combine Adagrad and Adadelta adaptive learning ability. Adam, which is a gradient-descent-based algorithm that mean and variance moment to do adaptive learning. Stochastic Variance Reduce Gradient (SVRG), which is an optimization SGD-based algorithm to accelerates the process toward converging by reducing the gradient. Semi Stochastic Gradient Descent (SSGD),which is a SGD-based algorithm that combine GD and SGD to accelerates the process toward converging by choosing one of the gradients at a time. Stochastic Recursive Gradient Algorithm (SARAH), which is an optimization algorithm similarly SVRG to accelerates the process toward converging by accumulated stochastic information. Stochastic Recursive Gradient Algorithm+ (SARAHPlus), which is a SARAH practical variant algorithm to accelerates the process toward converging provides a possibility of earlier termination.",
    "tags": null,
    "text_prefix": false,
    "timestamp": 1636230945,
    "version": "3.0"
  },
  "channeldata_version": 1,
  "feedstock": "r-graddescent-feedstock",
  "labels": [
    "main"
  ],
  "package": "r-graddescent-3.0-r41hc72bb7e_0.tar.bz2",
  "repodata": {
    "build": "r41hc72bb7e_0",
    "build_number": 0,
    "depends": [
      "r-base >=4.1,<4.2.0a0"
    ],
    "license": "GPL-2.0-or-later",
    "license_family": "GPL2",
    "md5": "dbf415b96cbdd5faa3c4aa501909c0e0",
    "name": "r-graddescent",
    "noarch": "generic",
    "sha256": "031fe189cb95329d0833a1a61f056e68d777aa16819d50cbe7b420b2c6dcc644",
    "size": 163355,
    "subdir": "noarch",
    "timestamp": 1636230945742,
    "version": "3.0"
  },
  "repodata_version": 1,
  "subdir": "noarch",
  "url": "https://github.com/conda-forge/releases/releases/download/noarch/r-graddescent-3.0-r41hc72bb7e_0.tar.bz2/r-graddescent-3.0-r41hc72bb7e_0.tar.bz2"
}