{
  "channeldata": {
    "activate.d": false,
    "binary_prefix": false,
    "deactivate.d": false,
    "description": null,
    "dev_url": null,
    "doc_source_url": null,
    "doc_url": null,
    "home": "https://lincolnmullen.com/software/tokenizers/",
    "icon_hash": null,
    "icon_url": null,
    "identifiers": null,
    "keywords": null,
    "license": "MIT",
    "post_link": false,
    "pre_link": false,
    "pre_unlink": false,
    "recipe_origin": null,
    "run_exports": {},
    "source_git_url": null,
    "source_url": [
      "https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.2.1.tar.gz",
      "https://cran.r-project.org/src/contrib/tokenizers_0.2.1.tar.gz"
    ],
    "subdirs": [
      "linux-ppc64le"
    ],
    "summary": "Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'.",
    "tags": null,
    "text_prefix": false,
    "timestamp": 1622089387,
    "version": "0.2.1"
  },
  "channeldata_version": 1,
  "feedstock": "r-tokenizers-feedstock",
  "labels": [
    "main"
  ],
  "package": "r-tokenizers-0.2.1-r41h6203a36_1002.tar.bz2",
  "repodata": {
    "build": "r41h6203a36_1002",
    "build_number": 1002,
    "depends": [
      "libgcc-ng >=9.3.0",
      "libstdcxx-ng >=9.3.0",
      "r-base >=4.1,<4.2.0a0",
      "r-rcpp >=0.12.3",
      "r-snowballc >=0.5.1",
      "r-stringi >=1.0.1"
    ],
    "license": "MIT",
    "license_family": "MIT",
    "md5": "de5fc77f063e4090ec5441d6cdd09a87",
    "name": "r-tokenizers",
    "sha256": "1be69e03b39c65b7e1d9ab5bd86d624686972ebe35aac7ccff3037bf8357770e",
    "size": 667151,
    "subdir": "linux-ppc64le",
    "timestamp": 1622089387432,
    "version": "0.2.1"
  },
  "repodata_version": 1,
  "subdir": "linux-ppc64le",
  "url": "https://github.com/conda-forge/releases/releases/download/linux-ppc64le/r-tokenizers-0.2.1-r41h6203a36_1002.tar.bz2/r-tokenizers-0.2.1-r41h6203a36_1002.tar.bz2"
}