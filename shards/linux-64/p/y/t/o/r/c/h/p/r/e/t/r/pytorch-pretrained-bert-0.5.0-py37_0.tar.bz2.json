{"channeldata":{"activate.d":false,"binary_prefix":false,"deactivate.d":false,"description":"This repository contains op-for-op PyTorch reimplementations, pre-trained models and fine-tuning examples for:   - Google's BERT model,   - OpenAI's GPT model,   - Google/CMU's Transformer-XL model, and   - OpenAI's GPT-2 model. These implementations have been tested on several datasets (see the examples) and should match the performances of the associated TensorFlow implementations (e.g. ~91 F1 on SQuAD for BERT, ~88 F1 on RocStories for OpenAI GPT and ~18.3 perplexity on WikiText 103 for the Transformer-XL).","dev_url":"https://github.com/huggingface/pytorch-pretrained-BERT","doc_source_url":null,"doc_url":"https://github.com/huggingface/pytorch-pretrained-BERT","home":"https://github.com/huggingface/pytorch-pretrained-BERT","icon_hash":null,"icon_url":null,"identifiers":null,"keywords":null,"license":"Apache-2.0","post_link":false,"pre_link":false,"pre_unlink":false,"recipe_origin":null,"run_exports":{},"source_git_url":null,"source_url":"https://pypi.io/packages/source/p/pytorch-pretrained-bert/pytorch_pretrained_bert-0.6.2.tar.gz","subdirs":["linux-64"],"summary":"PyTorch version of Google AI BERT model with script to load Google pre-trained models","tags":null,"text_prefix":true,"timestamp":1557485220,"version":"0.5.0"},"channeldata_version":1,"feedstock":null,"labels":["main"],"package":"pytorch-pretrained-bert-0.5.0-py37_0.tar.bz2","repodata":{"build":"py37_0","build_number":0,"constrains":["python_abi * *_cp37m"],"depends":["boto3","numpy","python >=3.7,<3.8.0a0","pytorch >=0.4.1","requests","tqdm"],"license":"Apache 2.0","license_family":"Apache","md5":"537db01640fe678e0a224f90fbae4898","name":"pytorch-pretrained-bert","sha256":"c6c388e1d8fe4f0f9fc9516180420e1034afb09d6169ef0cc2c61c69c8c21d0c","size":112942,"subdir":"linux-64","timestamp":1550109612263,"version":"0.5.0"},"repodata_version":1,"subdir":"linux-64","url":"https://conda.anaconda.org/conda-forge/linux-64/pytorch-pretrained-bert-0.5.0-py37_0.tar.bz2"}