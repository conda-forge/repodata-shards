{"channeldata":{"activate.d":false,"binary_prefix":true,"deactivate.d":false,"description":null,"dev_url":null,"doc_source_url":null,"doc_url":null,"home":"https://lincolnmullen.com/software/tokenizers/","icon_hash":null,"icon_url":null,"identifiers":null,"keywords":null,"license":"MIT","post_link":false,"pre_link":false,"pre_unlink":false,"recipe_origin":null,"run_exports":{},"source_git_url":null,"source_url":["https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.2.1.tar.gz","https://cran.r-project.org/src/contrib/tokenizers_0.2.1.tar.gz"],"subdirs":["linux-64"],"summary":"Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'.","tags":null,"text_prefix":false,"timestamp":1594728825,"version":"0.1.4"},"channeldata_version":1,"feedstock":null,"labels":["main"],"package":"r-tokenizers-0.1.4-r341hfc679d8_1.tar.bz2","repodata":{"build":"r341hfc679d8_1","build_number":1,"depends":["libgcc-ng >=4.9","libstdcxx-ng >=4.9","r-base >=3.4.1,<3.4.2.0a0","r-rcpp >=0.12.3","r-snowballc >=0.5.1","r-stringi >=1.0.1"],"license":"MIT","license_family":"MIT","md5":"6f8e95b4c965cfd5fad09efda3d8e9fa","name":"r-tokenizers","sha256":"32aa0c33626da40ac12cb92d9ca1e63e456c3a05c0b1e57b1ef8cacd899cb815","size":124833,"subdir":"linux-64","timestamp":1533204401299,"version":"0.1.4"},"repodata_version":1,"subdir":"linux-64","url":"https://conda.anaconda.org/conda-forge/linux-64/r-tokenizers-0.1.4-r341hfc679d8_1.tar.bz2"}