{"channeldata":{"activate.d":false,"binary_prefix":true,"deactivate.d":false,"description":null,"dev_url":null,"doc_source_url":null,"doc_url":null,"home":"https://lincolnmullen.com/software/tokenizers/","icon_hash":null,"icon_url":null,"identifiers":null,"keywords":null,"license":"MIT","post_link":false,"pre_link":false,"pre_unlink":false,"recipe_origin":null,"run_exports":{},"source_git_url":null,"source_url":["https://cran.r-project.org/src/contrib/Archive/tokenizers/tokenizers_0.2.1.tar.gz","https://cran.r-project.org/src/contrib/tokenizers_0.2.1.tar.gz"],"subdirs":["win-64"],"summary":"Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words.  The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for  fast yet correct tokenization in 'UTF-8'.","tags":null,"text_prefix":false,"timestamp":1594728825,"version":"0.1.4"},"channeldata_version":1,"feedstock":null,"labels":["main"],"package":"r-tokenizers-0.1.4-r3.3.2_0.tar.bz2","repodata":{"arch":"x86_64","build":"r3.3.2_0","build_number":0,"depends":["r-base 3.3.2*","r-rcpp >=0.12.3","r-snowballc >=0.5.1","r-stringi >=1.0.1"],"license":"MIT","license_family":"MIT","md5":"32d81ed8092d01dc2fd28b369417f549","name":"r-tokenizers","platform":"win","sha256":"5b086414a5a0fb841669e8b080f3a963c9e18d4ef5e7667ea4b80b6316d98001","size":83152,"subdir":"win-64","version":"0.1.4"},"repodata_version":1,"subdir":"win-64","url":"https://conda.anaconda.org/conda-forge/win-64/r-tokenizers-0.1.4-r3.3.2_0.tar.bz2"}